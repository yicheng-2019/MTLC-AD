{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5374e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = self.mllm_encoder.embeddings(word_tokens.to(self.device))\n",
    "        \n",
    "        prompt_enhance_embeddings = self.learnable_embeddings(\n",
    "            torch.arange(self.seq_max).to(self.device)).unsqueeze(0).repeat(\n",
    "            [len(texts), 1, 1])\n",
    "        prompt_enhance_tokens = torch.zeros(len(texts), 77).to(self.device)\n",
    "\n",
    "        for i in range(len(texts)):\n",
    "            ind = torch.argmax(word_tokens[i], -1)\n",
    "            prompt_enhance_embeddings[i, 0] = word_embedding[i, 0]\n",
    "            prompt_enhance_embeddings[i, self.learnable_prompt_len + 1: self.learnable_prompt_len + ind] = word_embedding[i, 1: ind]\n",
    "            prompt_enhance_embeddings[i, self.learnable_prompt_len + ind + self.learnable_prompt_len] = word_embedding[i, ind]\n",
    "            attention_mask[i, : self.learnable_prompt_len * 2 + ind + 1] = 1\n",
    "            prompt_enhance_tokens[i, self.learnable_prompt_len * 2 + ind] = word_tokens[i, ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886dba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    DebertaV2Tokenizer, DebertaV2Model,\n",
    "    AutoTokenizer, AutoModel\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4954c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DebertaV2Tokenizer.from_pretrained(\"/export/space0/qiu-y/model/deberta-v3-base\")\n",
    "seq_max = 77\n",
    "texts = [\n",
    "        'normal', 'abuse', 'arrest', 'arson', 'assault',\n",
    "        'burglary', 'explosion', 'fighting', 'roadAccidents',\n",
    "        'robbery', 'shooting', 'shoplifting', 'stealing', 'vandalism']\n",
    "\n",
    "encoded = tokenizer(\n",
    "            texts,\n",
    "            max_length=seq_max,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "word_tokens = encoded[\"input_ids\"]\n",
    "attention_mask = encoded[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9df498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "learnable_prompt_len = 10\n",
    "prompt_hidden_dim = 512\n",
    "\n",
    "word_embedding = torch.randn((14, 77, 512))\n",
    "learnable_embeddings = nn.Embedding(seq_max, prompt_hidden_dim) # [77, 512]\n",
    "prompt_enhance_embeddings = learnable_embeddings(torch.arange(seq_max)).unsqueeze(0).repeat([len(texts), 1, 1])  # [14, 77, 512]\n",
    "prompt_enhance_tokens = torch.zeros(len(texts), 77) # [14, 77]\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    ind = torch.argmax(word_tokens[i], -1)\n",
    "    # print(prompt_enhance_embeddings[i, 0])\n",
    "    print(word_embedding[i, 0].shape)\n",
    "    prompt_enhance_embeddings[i, 0] = word_embedding[i, 0]  # 复制word_embedding中第一个向量到 prompt_enhance_embeddings的第一个位置\n",
    "    prompt_enhance_embeddings[i, learnable_prompt_len + 1: learnable_prompt_len + ind] = word_embedding[i, 1: ind]\n",
    "    prompt_enhance_embeddings[i, learnable_prompt_len + ind + learnable_prompt_len] = word_embedding[i, ind]\n",
    "    attention_mask[i, : learnable_prompt_len * 2 + ind + 1] = 1\n",
    "    prompt_enhance_tokens[i, learnable_prompt_len * 2 + ind] = word_tokens[i, ind]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45c24b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词器定义的[CLS] token是: [CLS]\n",
      "它的ID是: 1\n",
      "------------------------------\n",
      "原始文本: 'hello world'\n",
      "编码后的Token ID: [1, 12018, 447, 2]\n",
      "解码回Token: ['[CLS]', '▁hello', '▁world', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "# 使用您自己的模型路径\n",
    "vid_mllm_ckpt = \"/export/space0/qiu-y/model/deberta-v3-base\" \n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(vid_mllm_ckpt)\n",
    "\n",
    "# --- 验证部分 ---\n",
    "\n",
    "# 1. 直接查看分词器中定义的[CLS] token是什么\n",
    "print(f\"分词器定义的[CLS] token是: {tokenizer.cls_token}\")\n",
    "print(f\"它的ID是: {tokenizer.cls_token_id}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. 观察分词器如何处理一段普通文本\n",
    "text = \"hello world\"\n",
    "encoded = tokenizer(text)\n",
    "\n",
    "print(f\"原始文本: '{text}'\")\n",
    "print(f\"编码后的Token ID: {encoded['input_ids']}\")\n",
    "\n",
    "# 3. 将ID解码回Token，看得更清楚\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'])\n",
    "print(f\"解码回Token: {tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
